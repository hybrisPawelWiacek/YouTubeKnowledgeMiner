<?xml version="1.0" encoding="utf-8" ?><transcript><text start="0.08" dur="2.998">Martin, it seems like AI is everywhere these days.</text><text start="3.08" dur="5.816">Finally, we have a computer that actually understands my language instead of me having to learn its language.</text><text start="8.9" dur="2.059">A system that understands me.</text><text start="10.96" dur="5.656">For instance, what if I&amp;#39;m looking to buy a new car and I need to do some research on the alternatives?</text><text start="16.62" dur="4.674">Yeah, you could tell the chatbot to act as a car expert and then you can ask it,</text><text start="21.294" dur="3.914">what would be the difference in cost to operate a gas powered car versus</text><text start="25.208" dur="4.534">a hybrid car versus an EV car and then get guidance on the decision.</text><text start="29.75" dur="6.436">And if it helped me find a rebate from the power company, it could pay for itself in just one instance,</text><text start="36.19" dur="2.965">and if I enjoyed tinkering and DIY projects,</text><text start="39.155" dur="5.42">wouldn&amp;#39;t it be cool to learn how the technology works and host my very own instance of all of this?</text><text start="44.58" dur="1.019">Yeah, very cool.</text><text start="45.6" dur="1.759">And in fact, we have a colleague,</text><text start="47.36" dur="5.077">Robert Murray, who has done just that with equipment in his own home office.</text><text start="52.44" dur="5.717">Wait, you mean without a server farm of GPUs that dim the lights every time you ask it to do something?</text><text start="58.16" dur="0.819">Absolutely.</text><text start="58.98" dur="3.458">So let&amp;#39;s bring him in to tell us how he did it.</text><text start="62.44" dur="3.378">Today, requests to Generative AI typically connect to an AI model</text><text start="65.82" dur="1.979">hosted somewhere on a cloud,</text><text start="67.8" dur="3.19">but Robert here has built an infrastructure</text><text start="70.99" dur="3.218">to host AI models like Llama 3 and IBM&amp;#39;s Granite</text><text start="74.208" dur="5.79">on his own personal infrastructure. So Robert, I want to understand how you did this.</text><text start="80" dur="0.731">Absolutely.</text><text start="80.731" dur="5.106">o let&amp;#39;s start with this box, which represents your computer at home.</text><text start="85.84" dur="2.998">So tell me sort of the stack that you built here.</text><text start="88.84" dur="2.748">Sure. So I started with Windows 11.</text><text start="91.59" dur="0.864">All right, so it&amp;#39;s just a straight up.</text><text start="92.454" dur="1.594">Because I have it.</text><text start="94.05" dur="1.079">Yeah, ok.</text><text start="95.13" dur="1.932">That was the reason, just because it&amp;#39;s there.</text><text start="97.062" dur="1.586">It&amp;#39;s there.</text><text start="98.65" dur="2.378">OK, so you&amp;#39;ve got Wins 11, and then what&amp;#39;s on top of that?</text><text start="101.03" dur="4.178">Well, I unleashed WSL2.</text><text start="105.21" dur="2.851">Now you&amp;#39;re gonna have to tell me what WSL2 does.</text><text start="108.062" dur="2.826">It&amp;#39;s basically Linux on Windows.</text><text start="111.57" dur="3.305">I&amp;#39;m going to think that there&amp;#39;s probably a virtualization layer coming.</text><text start="114.875" dur="3.491">Yes, there definitely is and that is Docker.</text><text start="118.37" dur="2.799">Ok, Docker is running on top of all of this.</text><text start="121.89" dur="7.576">Now, we need some AI models. So where did you get your AI models from?</text><text start="129.47" dur="3.079">I pulled them down from Ollama.com.</text><text start="132.55" dur="5.933">OK, so if we take a look at the AI models, what are some of the models that you actually took?</text><text start="138.483" dur="1.697">Oh, so I started with Granite.</text><text start="140.18" dur="1.846">Right, IBM&amp;#39;s granite model, yeah.</text><text start="142.459" dur="0.5">Llama,</text><text start="144.65" dur="2.698">and there&amp;#39;s so many other models that you can pull down.</text><text start="147.35" dur="0.655">Yeah.</text><text start="148.005" dur="1.574">They&amp;#39;re there, Open source.</text><text start="149.58" dur="1.579">A whole bunch of open source models.</text><text start="151.16" dur="1.639">Okay, so we&amp;#39;ve got a Docker machine</text><text start="152.8" dur="3.258">here with Windows 11, WSL2.</text><text start="156.06" dur="2.678">You&amp;#39;ve downloaded these models from Ollama.</text><text start="158.74" dur="2.499">Is this now the solution?</text><text start="161.24" dur="3.517">Well, I actually can use this. I can run all this right from the command line.</text><text start="164.76" dur="4.417">Wow. Okay. So you can open a terminal window and then start chatting with Llama or Granite.</text><text start="169.18" dur="1.47">Yes. Very, very fast.</text><text start="170.65" dur="3.139">But most of the AM models that are cloud hosted,</text><text start="173.789" dur="5.986">you do that on a chat interface, a UI. So how are you able to add a UI to all of this?</text><text start="179.78" dur="1.239">Docker containers.</text><text start="181.02" dur="1.319">Ah, okay, all right.</text><text start="182.34" dur="2.738">So let&amp;#39;s put some Docker containers in. What did you have for the UI?</text><text start="185.08" dur="1.819">I used Open WebUI.</text><text start="186.9" dur="6.263">It&amp;#39;s one of the many solutions that a person could use, but I found this to be extraordinarily helpful.</text><text start="193.163" dur="0.5">Ok.</text><text start="193.663" dur="0.903">It&amp;#39;s easy to use.</text><text start="194.566" dur="3.614">Yeah! So with Open WebUI, you can just open up a browser</text><text start="198.18" dur="3.506">and then chat with the model, pick the model you want, and send requests to it.</text><text start="201.69" dur="4.917">And there I was, and that&amp;#39;s what I was working with for a long time right out of my home.</text><text start="206.61" dur="2.779">But what if you&amp;#39;re on the go?</text><text start="209.39" dur="3.467">Well, that&amp;#39;s where another container comes in.</text><text start="212.859" dur="1.18">Okay, what have you got here?</text><text start="214.04" dur="5.447">So it&amp;#39;s a VPN container configured with my own domain.</text><text start="219.49" dur="3.598">All right, so what can access this guy?</text><text start="224.539" dur="0.954">This.</text><text start="225.493" dur="1.58">Ah, ok, your phone.</text><text start="227.57" dur="9.535">So now I am able to access my system from my phone or basically any internet connection.</text><text start="237.11" dur="0.889">It&amp;#39;s awesome.</text><text start="238" dur="1.139">How very cool.</text><text start="239.14" dur="2.019">All right, well, let&amp;#39;s say that</text><text start="241.16" dur="1.659">I wanted to actually replicate what</text><text start="242.82" dur="1.499">you&amp;#39;ve done here and build it.</text><text start="244.32" dur="4.129">I&amp;#39;m gonna ask you about this server itself. What are the system requirements?</text><text start="248.449" dur="2.966">So let&amp;#39;s start with RAM. How much RAM do I need for this?</text><text start="251.42" dur="3.488">I would recommend at least 8 gigabytes.</text><text start="254.91" dur="1.108">8 gigabytes.</text><text start="256.019" dur="0.98">That&amp;#39;s not much.</text><text start="257" dur="0.969">How much do you actually use?</text><text start="257.97" dur="2.268">Well, I&amp;#39;m using 96</text><text start="260.24" dur="2.578">OK, slightly above the minimum requirement.</text><text start="262.82" dur="3.378">Absolutely. All right, so let&amp;#39;s RAM. What about storage?</text><text start="266.2" dur="3.158">Storage, I would recommend having at least one terabyte.</text><text start="269.36" dur="2.718">OK, because some of these models can get pretty big.</text><text start="272.08" dur="0.799">Yes, they can.</text><text start="272.88" dur="6.686">Now, these models come in different sizes. So what&amp;#39;s parameter count sizes we&amp;#39;re using with Granite and Llama?</text><text start="279.57" dur="1.819">I&amp;#39;m using anywhere between 7 and</text><text start="281.39" dur="1.859">14 billion parameters.</text><text start="283.25" dur="1.75">7 to 14 billion, okay.</text><text start="286.35" dur="1.979">I have run up to 70.</text><text start="288.33" dur="0.839">70?</text><text start="289.17" dur="1.319">How did that work out?</text><text start="290.49" dur="1.219">Slow.</text><text start="291.71" dur="0.979">I can imagine.</text><text start="292.69" dur="0.879">OK.</text><text start="293.57" dur="2.059">So the other thing that people</text><text start="295.63" dur="3.978">often talk about in terms of system requirements are GPUs.</text><text start="299.61" dur="2.657">So should I be using GPUs for this?</text><text start="302.267" dur="1.882">Well. My initial configuration.</text><text start="304.15" dur="1.219">I had no GPUs,</text><text start="306.348" dur="0.635">but</text><text start="308.036" dur="1.366">more GPUs the better</text><text start="309.712" dur="1.521">The more, the better, right.</text><text start="311.58" dur="2.039">So, we&amp;#39;ve got this self-contained</text><text start="313.62" dur="4.284">solution now, and it&amp;#39;s got me thinking that when I talk to a large language model,</text><text start="317.904" dur="3.631">I often want to provide it documentation in order to chat with that document.</text><text start="321.54" dur="0.859">Absolutely.</text><text start="322.4" dur="2.301">Now, if I&amp;#39;m using a cloud-based model, I need</text><text start="324.701" dur="5.254">to take my document and upload it to somebody else&amp;#39;s server so that the AI model can see it.</text><text start="329.96" dur="1.818">I take it that you have a better solution to that.</text><text start="331.78" dur="2.329">I do. I use my own NAS system.</text><text start="334.11" dur="3.788">Okay, so you have a NAS server setup.</text><text start="337.9" dur="2.772">And from that NAS system, I pull in my documents,</text><text start="340.672" dur="6.283">pull them into the open web UI, and chat away. And I&amp;#39;m doing it every single day.</text><text start="346.96" dur="1.419">So Robert, the other thing I like</text><text start="348.38" dur="1.259">about this architecture is at least</text><text start="349.64" dur="2.858">to my mind, this looks like a really secure solution.</text><text start="352.5" dur="6.416">Hold the phone there just a second, nice job AI guy, but let&amp;#39;s really look at the security on this Robert.</text><text start="358.92" dur="7.296">First of all, I think it is a good job here and I think you&amp;#39;ve put in some features that will help preserve security and privacy,</text><text start="366.22" dur="7.24">but let&amp;#39;s take a look at what some of those are because what you don&amp;#39;t want is your data is our data,</text><text start="373.46" dur="4.254">we want your data is your data, not your data is our business model.</text><text start="377.72" dur="3.806">So how do we make sure that we&amp;#39;re not falling into the same trap</text><text start="381.526" dur="6.988">that a lot of those other chatbots that are free apps on the app store that you can download aren&amp;#39;t falling into.</text><text start="388.52" dur="2.401">Well, first off, I put it on my own hardware.</text><text start="390.921" dur="1.377">Yeah, exactly.</text><text start="392.3" dur="5.157">So I see that very clearly. It&amp;#39;s on your hardware, so you control the infrastructure.</text><text start="397.46" dur="2.458">You can decide when to turn the thing on and off.</text><text start="399.92" dur="1.919">It&amp;#39;s your data on your system.</text><text start="401.84" dur="1.379">So that&amp;#39;s the first point.</text><text start="403.22" dur="0.839">Absolutely.</text><text start="404.06" dur="4.937">Yeah, and then also it looks like that you included a private data store.</text><text start="409" dur="4.734">So now it&amp;#39;s not your information is training somebody else&amp;#39;s model,</text><text start="413.734" dur="4.141">and you&amp;#39;re pulling information that might be poisoned or anything like that.</text><text start="417.88" dur="1.8">You have some control over that as well.</text><text start="419.68" dur="3.996">Yes, and interesting enough, that&amp;#39;s what&amp;#39;s actually got me started on this whole path.</text><text start="423.68" dur="3.388">By having a NAS, I wanted my data to be my data.</text><text start="427.07" dur="4.837">And data is the real core of an AI system anyway, so that makes a lot of sense.</text><text start="431.91" dur="2.298">Also, I noticed some open source components.</text><text start="434.21" dur="4.223">So you&amp;#39;ve got one right here, you&amp;#39;ve got open source models here as well.</text><text start="438.433" dur="6.72">And that&amp;#39;s a good idea, because instead of proprietary stuff, in these cases, at least we have an idea</text><text start="445.153" dur="4.358">that the worldwide open source community has had a chance to look at this and vet it.</text><text start="449.511" dur="5.828">Now granted, there&amp;#39;s a lot of information to be vetted, so it&amp;#39;s not trivial, no guarantees.</text><text start="455.35" dur="5.696">Maybe it&amp;#39;s a little more secure because more people have had a chance to look at what&amp;#39;s actually happening under the covers.</text><text start="461.05" dur="0.799">Agreed.</text><text start="461.85" dur="4.301">And then also I notice you want to be able to access this from anywhere,</text><text start="466.151" dur="5.433">which is one of the really cool aspects and we want to make sure that that access is also secured.</text><text start="471.59" dur="2.059">So I see you put a VPN over</text><text start="473.65" dur="4.278">here so that you can connect your phone in and do that securely.</text><text start="477.93" dur="4.507">And how are you making sure everybody else in the world can&amp;#39;t connect their phone in here as well?</text><text start="482.44" dur="1">multi-factor.</text><text start="484.21" dur="4.078">multi-factor authentication, and now we know it&amp;#39;s really you,</text><text start="488.29" dur="3.378">and we know the information is exchanged in a secure way.</text><text start="491.67" dur="3.258">So a lot of features that you put in here, I think it&amp;#39;s a nice job.</text><text start="494.93" dur="0.614">Thank you.</text><text start="495.544" dur="6.109">Yeah. And one other thing to think about, because these components, we really don&amp;#39;t know what all of them would do,</text><text start="501.653" dur="3.514">it is still possible that one of these things could be</text><text start="505.167" dur="4.874">phoning home and sending data to the mothership, even without your knowledge.</text><text start="510.05" dur="3.849">So one of the things that might be useful is put a network tap on your home network,</text><text start="513.899" dur="3.641">and then that way you could see if there are any outbound connections from this,</text><text start="517.54" dur="2.798">because there shouldn&amp;#39;t be based upon the way you&amp;#39;ve built that.</text><text start="520.34" dur="1.179">Well, that&amp;#39;s a really great idea,</text><text start="521.52" dur="1.408">Jeff. I&amp;#39;m going to have to look into that.</text><text start="522.929" dur="2.819">Okay, there you go with the improvements for version two.</text><text start="525.75" dur="0.839">Hey, Jeff.</text><text start="526.59" dur="2.418">Oh, hey, Martin. Nice to have you back.</text><text start="529.01" dur="3.647">Yeah, it seems like Robert&amp;#39;s really done some nice work with this, don&amp;#39;t you think?</text><text start="532.66" dur="4.285">For sure. It just goes to show that you can now run sophisticated</text><text start="536.945" dur="4.031">AI models on a home computer to build a personal chatbot.</text><text start="540.98" dur="3.998">Yeah. Something like that would have been science fiction just a few short years ago,</text><text start="545.251" dur="3.697">but now it&amp;#39;s available to anyone who really wants to spend the time to assemble it all.</text><text start="548.948" dur="6.513">Right. and you&amp;#39;d like. So much more about a technology by really digging into it and getting your hands dirty with it.</text><text start="555.47" dur="4.117">Yeah, and by the looks of your hands, you&amp;#39;ve been doing a lot of digging because those things are filthy,</text><text start="559.95" dur="4.973">and the added bonus is that you end up with a better assurance that your data is your data</text><text start="565" dur="4.944">because you have more control and you can ensure that privacy is protected in the process.</text><text start="569.95" dur="4.278">Spoken like a true security guy that you are, Jeff.</text><text start="574.23" dur="3.404">All right, so you&amp;#39;ve seen Robert&amp;#39;s approach.</text><text start="577.634" dur="5.271">So how would you, dear viewer, do anything differently to make the system even better?</text><text start="582.91" dur="1">Let us know in the comments.</text></transcript>